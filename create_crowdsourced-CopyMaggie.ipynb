{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MeteoSwiss app hail crowdsourced reports toolbox\n",
    "\n",
    "This jupyter notebook contains several functions to work with the crowdsourced hail reports from the MeteoSwiss application\n",
    "\n",
    "Step 1 downloads the data from the web service, apply some basic filtering and compute additional fields.\n",
    "\n",
    "Step 2 finds the maximum value of several radar products over different time windows and spatial radius around the report time and location. It creates bash scripts which further call add_rad_var_crowd.py and get_archived_data_JKO.py. The list of variables can be changed in the notebook and the list of time steps and radius can be changed in add_rad_var_crowd.py.\n",
    "\n",
    "Step 3 and 4 group the radar variables and merge with previous data.\n",
    "\n",
    "Step 5 can be used to define regions based on population density or Swiss coordinates to further filter the reports (and use the numpy arrays as filters for corresponding cartesian radar variables if needed).\n",
    "\n",
    "Step 6 is used to create spatio-temporal clusters of reports using the library st-dbscan. This step doesn't require Step 2 to 4 to be run before (step 5 is not necessary unless a focus on a specific region is required.).\n",
    "\n",
    "Step 7 is used to merge back all the clusters information together with the original reports dataframe. The resulting dataframe is called crowd_consolidated_2015-05-02_2023-10-15.pkl. Some filtering examples are listed at the end of step 7.\n",
    "\n",
    "### Please note that all scripts work properly (hopefully) but their code are clearly not optimised.\n",
    "#### This script has been updated by Margaret from October 2024\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "from st_dbscan import ST_DBSCAN\n",
    "import time\n",
    "import geopandas as gpd\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: download raw data and add filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app-prod-ws.meteoswiss-app.ch/v1/hagel/A8uWrl4da/data?start=2024-08-29&end=2024-08-30\n",
      "https://app-prod-ws.meteoswiss-app.ch/v1/hagel/A8uWrl4da/data?start=2024-08-30&end=2024-08-31\n",
      "https://app-prod-ws.meteoswiss-app.ch/v1/hagel/A8uWrl4da/data?start=2024-08-31&end=2024-09-01\n",
      "https://app-prod-ws.meteoswiss-app.ch/v1/hagel/A8uWrl4da/data?start=2024-09-01&end=2024-09-02\n",
      "https://app-prod-ws.meteoswiss-app.ch/v1/hagel/A8uWrl4da/data?start=2024-09-02&end=2024-09-03\n",
      "https://app-prod-ws.meteoswiss-app.ch/v1/hagel/A8uWrl4da/data?start=2024-09-03&end=2024-09-04\n",
      "https://app-prod-ws.meteoswiss-app.ch/v1/hagel/A8uWrl4da/data?start=2024-09-04&end=2024-09-05\n",
      "https://app-prod-ws.meteoswiss-app.ch/v1/hagel/A8uWrl4da/data?start=2024-09-05&end=2024-09-06\n",
      "https://app-prod-ws.meteoswiss-app.ch/v1/hagel/A8uWrl4da/data?start=2024-09-06&end=2024-09-07\n",
      "https://app-prod-ws.meteoswiss-app.ch/v1/hagel/A8uWrl4da/data?start=2024-09-07&end=2024-09-08\n",
      "https://app-prod-ws.meteoswiss-app.ch/v1/hagel/A8uWrl4da/data?start=2024-09-08&end=2024-09-09\n",
      "https://app-prod-ws.meteoswiss-app.ch/v1/hagel/A8uWrl4da/data?start=2024-09-09&end=2024-09-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_90040/3265106371.py:80: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  raw_data['month'] = raw_data['Time'].dt.to_period('M')\n",
      "/tmp/ipykernel_90040/3265106371.py:81: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  raw_data['year'] = raw_data['Time'].dt.to_period('Y')\n",
      "/tmp/ipykernel_90040/3265106371.py:113: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  raw_data['Time_m15'] = (raw_data['Time'] - datetime.timedelta(minutes=15)).dt.ceil(freq='5T').dt.strftime('%Y%m%d%H%M%S')\n",
      "/tmp/ipykernel_90040/3265106371.py:114: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  raw_data['Time_p15'] = (raw_data['Time'] + datetime.timedelta(minutes=15)).dt.floor(freq='5T').dt.strftime('%Y%m%d%H%M%S')\n",
      "/tmp/ipykernel_90040/3265106371.py:115: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  raw_data['Time_m10'] = (raw_data['Time'] - datetime.timedelta(minutes=10)).dt.ceil(freq='5T').dt.strftime('%Y%m%d%H%M%S')\n",
      "/tmp/ipykernel_90040/3265106371.py:116: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  raw_data['Time_p10'] = (raw_data['Time'] + datetime.timedelta(minutes=10)).dt.floor(freq='5T').dt.strftime('%Y%m%d%H%M%S')\n",
      "/tmp/ipykernel_90040/3265106371.py:117: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  raw_data['Time_m5'] = (raw_data['Time'] - datetime.timedelta(minutes=5)).dt.ceil(freq='5T').dt.strftime('%Y%m%d%H%M%S')\n",
      "/tmp/ipykernel_90040/3265106371.py:118: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  raw_data['Time_p5'] = (raw_data['Time'] + datetime.timedelta(minutes=5)).dt.floor(freq='5T').dt.strftime('%Y%m%d%H%M%S')\n"
     ]
    }
   ],
   "source": [
    "# Download the crowdsourced between date_from and date_to, perform data cleaning, add new fields and filters\n",
    "date_from = \"2024-08-29\"\n",
    "date_to = \"2024-09-10\"\n",
    "\n",
    "dates = pd.date_range(date_from,date_to,freq='d').strftime('%Y-%m-%d').tolist()\n",
    "\n",
    "base_url = \"https://app-prod-ws.meteoswiss-app.ch/v1/hagel/A8uWrl4da/data?start=\"\n",
    "\n",
    "raw_data = pd.DataFrame()\n",
    "\n",
    "#We proceed day by day because download is limited to 10 days max\n",
    "for idx in range(len((dates))-1):\n",
    "    url = base_url + dates[idx] + \"&end=\" + dates[idx+1]\n",
    "    print(url)\n",
    "    temp = pd.read_csv(url)\n",
    "    raw_data = pd.concat([raw_data,temp])\n",
    "\n",
    "    \n",
    "#remove last column, artifact without data\n",
    "raw_data = raw_data.iloc[:,:-1]\n",
    "\n",
    "#remove potential duplicates\n",
    "raw_data = raw_data.drop_duplicates(subset=[\"HagelUserId\", \"x\",\"y\", \"Time\", \"Type\"])\n",
    "\n",
    "raw_data = raw_data.rename(columns={'Type': 'size', 'HagelUserId': 'ID'})\n",
    "\n",
    "convert_dict = {'ID':str, 'x':int, 'y':int, 'size':int, 'CustomTime':str,\n",
    "                'CustomLocation':str, 'OsVersion':str, 'AppVersion':str}\n",
    " \n",
    "raw_data = raw_data.astype(convert_dict)\n",
    "\n",
    "#restrict area to Switerzland: (30000,310000,470000,840000) # minCHX,maxCHX,minCHY,maxCHY\n",
    "raw_data = raw_data.loc[((raw_data['x']>=470000) & (raw_data['x']<840000) & (raw_data['y']>=30000) & (raw_data['y']<310000))]\n",
    "\n",
    "#remove default location when GPS not active\n",
    "raw_data = raw_data.loc[~((raw_data['x']==717144) & (raw_data['y']==95751))]\n",
    "raw_data = raw_data.loc[~((raw_data['x']==537969) & (raw_data['y']==152459))]\n",
    "raw_data = raw_data.loc[~((raw_data['x']==683472) & (raw_data['y']==247852))]\n",
    "\n",
    "#convert from unix epoch time format to usual date time\n",
    "raw_data['Time'] = pd.to_datetime(raw_data['Time']/1000,unit='s',utc=True)\n",
    "raw_data['SubmissionTime'] = pd.to_datetime(raw_data['SubmissionTime']/1000,unit='s',utc=True)\n",
    "raw_data['T_diff'] = (raw_data['SubmissionTime'] - raw_data['Time']).dt.total_seconds()\n",
    "\n",
    "#add size labels and categories\n",
    "conditions = [\n",
    "        (raw_data['size'] == 10),(raw_data['size'] == 11),(raw_data['size'] == 12),(raw_data['size'] == 13),\n",
    "        (raw_data['size'] == 14),(raw_data['size'] == 15),(raw_data['size'] == 16)]\n",
    "\n",
    "conditions2 = [\n",
    "        (raw_data['size'] == 10) | (raw_data['size'] == 0),\n",
    "        (raw_data['size'] == 11) | (raw_data['size'] == 1) | (raw_data['size'] == 12),\n",
    "        (raw_data['size'] == 2) | (raw_data['size'] == 13),\n",
    "        (raw_data['size'] == 3) | (raw_data['size'] == 14),\n",
    "        (raw_data['size'] == 15) | (raw_data['size'] == 16) | (raw_data['size'] == 4)]\n",
    "\n",
    "# create a list of the values we want to assign for each condition\n",
    "values = ['No hail', '< coffee bean', 'Coffee bean', '1 CHF', '5 CHF', 'Golf ball', 'Tennis ball']\n",
    "values2 = ['No hail', '< 10 [mm]', '10 [mm]', '20 [mm]', '30 [mm]', '50 [mm]', '>70 [mm]']\n",
    "values3 = ['No hail', '<15 [mm]', '15-27 [mm]', '27-37 [mm]', '>37 [mm]']\n",
    "values4 = [0,1,2,3,4]\n",
    "\n",
    "# create a new column and use np.select to assign values to it using our lists as arguments\n",
    "raw_data['size_text'] = np.select(conditions, values, default='Unknown')\n",
    "raw_data['size_text_new'] = np.select(conditions, values2, default='Unknown')\n",
    "raw_data['size_range_all'] = np.select(conditions2, values3, default='Unknown')\n",
    "raw_data['size_range_all_num'] = np.select(conditions2, values4, default=-1)  # Ensure this matches integer type  \n",
    "# The default='Unknown' ensures there is a value for any unmatched conditions.     \n",
    "\n",
    "\n",
    "raw_data['day'] = raw_data['Time'].dt.date\n",
    "raw_data['month'] = raw_data['Time'].dt.to_period('M')\n",
    "raw_data['year'] = raw_data['Time'].dt.to_period('Y')\n",
    "raw_data['Timer'] = raw_data['Time'].dt.round(\"5min\")\n",
    "raw_data['xr'] = round(raw_data['x']/500.0)*500.0\n",
    "raw_data['yr'] = round(raw_data['y']/500.0)*500.0\n",
    "\n",
    "raw_data['month_name'] = raw_data['Time'].dt.month_name()\n",
    "raw_data['month_number'] = raw_data['Time'].dt.month\n",
    "raw_data['hour'] = raw_data['Time'].dt.hour\n",
    "raw_data['Time_only'] = raw_data['Time'].dt.time\n",
    "    \n",
    "cond_season = [raw_data['month_number'].isin([12,1,2]), raw_data['month_number'].isin([3,4,5]),\n",
    "                raw_data['month_number'].isin([6,7,8]), raw_data['month_number'].isin([9,10,11])]\n",
    "val_season = ['DJF', 'MAM', 'JJA', 'SON']\n",
    "raw_data['season_name'] = np.select(cond_season, val_season, default='Unknown')\n",
    "\n",
    "# rounding for further grouping\n",
    "raw_data['Time_30s'] = raw_data['Time'].dt.round(\"30s\")\n",
    "raw_data['Time_1min'] = raw_data['Time'].dt.round(\"1min\")\n",
    "raw_data['Time_2min'] = raw_data['Time'].dt.round(\"2min\")\n",
    "raw_data['Time_1h'] = raw_data['Time'].dt.round(\"1h\")\n",
    "raw_data['Time_30s'] = raw_data['Time'].dt.round(\"30s\")\n",
    "\n",
    "raw_data['x_1000m'] = round(raw_data['x'] / 1000) * 1000\n",
    "raw_data['x_2000m'] = round(raw_data['x'] / 2000) * 2000\n",
    "\n",
    "raw_data['y_1000m'] = round(raw_data['y'] / 1000) * 1000\n",
    "raw_data['y_2000m'] = round(raw_data['y'] / 2000) * 2000\n",
    "\n",
    "\n",
    "#timestamps for radar data\n",
    "raw_data['Timestamp'] = raw_data['Timer'].dt.strftime('%Y%m%d%H%M%S')\n",
    "raw_data['Time_m15'] = (raw_data['Time'] - datetime.timedelta(minutes=15)).dt.ceil(freq='5T').dt.strftime('%Y%m%d%H%M%S')\n",
    "raw_data['Time_p15'] = (raw_data['Time'] + datetime.timedelta(minutes=15)).dt.floor(freq='5T').dt.strftime('%Y%m%d%H%M%S')\n",
    "raw_data['Time_m10'] = (raw_data['Time'] - datetime.timedelta(minutes=10)).dt.ceil(freq='5T').dt.strftime('%Y%m%d%H%M%S')\n",
    "raw_data['Time_p10'] = (raw_data['Time'] + datetime.timedelta(minutes=10)).dt.floor(freq='5T').dt.strftime('%Y%m%d%H%M%S')\n",
    "raw_data['Time_m5'] = (raw_data['Time'] - datetime.timedelta(minutes=5)).dt.ceil(freq='5T').dt.strftime('%Y%m%d%H%M%S')\n",
    "raw_data['Time_p5'] = (raw_data['Time'] + datetime.timedelta(minutes=5)).dt.floor(freq='5T').dt.strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "# Flag reports submitted 30 minutes after event (Barras et al., 2019)\n",
    "raw_data['Flag_30min'] = 0\n",
    "raw_data.loc[raw_data['T_diff'] > 1800, 'Flag_30min'] = 1\n",
    "\n",
    "\n",
    "# Flag reports submitted by same user on same day with more than 3 CustomLocation=TRUE (Barras et al., 2019)\n",
    "temp = raw_data.groupby(['ID','day','CustomLocation'])\n",
    "raw_data = raw_data.set_index(['ID','day','CustomLocation'])\n",
    "raw_data['N_day_ID_custom'] = temp.size()\n",
    "raw_data = raw_data.reset_index()\n",
    "raw_data['Flag_day_ID_3_custom'] = 0\n",
    "raw_data.loc[(raw_data['N_day_ID_custom']>3) & (raw_data['CustomLocation']==True),'Flag_day_ID_3_custom'] = 1\n",
    "\n",
    "# Flag reports if same user submitted smallest and largest size within 2 min (Barras et al., 2019)\n",
    "temp = raw_data.groupby(['ID','Time_2min'])\n",
    "raw_data['Flag_2min_ID_S_XL'] = (((temp['size'].transform(lambda x: x.eq(4).any())) & (temp['size'].transform(lambda x: x.isin([0,1]).any())))\n",
    "                        | ((temp['size'].transform(lambda x: x.eq(16).any())) & (temp['size'].transform(lambda x: x.isin([10,11,12]).any()))))\n",
    "raw_data['Flag_2min_ID_S_XL'] = raw_data['Flag_2min_ID_S_XL'].astype(int)\n",
    "\n",
    "# Flag reports if same user submitted 3 reports, including largest size within 1 hour (Barras et al., 2019)\n",
    "temp = raw_data.groupby(['ID','Time_1h'])\n",
    "raw_data['Flag_1h_ID_3size_XL'] = (((temp['size'].transform(lambda x: x.eq(4).any())) & (temp['size'].transform(lambda x: x.nunique() > 2)))\n",
    "                        | ((temp['size'].transform(lambda x: x.eq(16).any())) & (temp['size'].transform(lambda x: x.nunique() > 2))))\n",
    "raw_data['Flag_1h_ID_3size_XL'] = raw_data['Flag_1h_ID_3size_XL'].astype(int)\n",
    "\n",
    "# Blacklist flag (Barras et al., 2019)\n",
    "raw_data['Flag_blacklist'] = 0\n",
    "raw_data.loc[(raw_data['Flag_day_ID_3_custom']==1) | (raw_data['Flag_2min_ID_S_XL']==1) |\n",
    "              (raw_data['Flag_1h_ID_3size_XL']==1),'Flag_blacklist'] = 1\n",
    "\n",
    "# Flag reports if same user submitted more than 4 on same day\n",
    "temp = raw_data.groupby(['ID','day'])\n",
    "raw_data = raw_data.set_index(['ID','day'])\n",
    "raw_data['N_day_ID'] = temp.size()\n",
    "raw_data = raw_data.reset_index()\n",
    "raw_data['Flag_N_day_ID_4'] = 0\n",
    "raw_data.loc[raw_data['N_day_ID']>4,'Flag_N_day_ID_4'] = 1\n",
    "\n",
    "# Save as csv\n",
    "raw_data.to_csv('crowd_reports_%s_%s.csv' % (date_from, date_to))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'time_dt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_42681/2682894675.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Timo's filter for 4th dimension clustering by hail size category\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mraw_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"time_int\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_dt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"int64\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"time_int\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"time_int\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"time_int\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'chx'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'chy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'time_int'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrowd_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mper_category_scaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_dist_p_category\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md_min\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# km,km, s, #size_categories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/micromamba/envs/jerome/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6295\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6296\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6297\u001b[0m         ):\n\u001b[1;32m   6298\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6299\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'time_dt'"
     ]
    }
   ],
   "source": [
    "# Timo's filter for 4th dimension clustering by hail size category\n",
    "\n",
    "# Does not work at the moment!\n",
    "# Variable names need to be adjusted to match the dataset**\n",
    "\n",
    "raw_data[\"time_int\"] = raw_data.time_dt.astype(\"int64\")//10**9  \n",
    "raw_data[\"time_int\"] = raw_data[\"time_int\"] - raw_data[\"time_int\"].min()\n",
    "X = raw_data[['chx','chy','time_int','size']].values\n",
    "X = sc.crowd_process.per_category_scaler(X,min_dist_p_category=[dx,dx,d_min*60,d_size]) # km,km, s, #size_categories\n",
    "dbscan = DBSCAN(eps=1,min_samples=5)\n",
    "dbscan.fit(X)\n",
    "raw_data['cluster'] = dbscan.labels_\n",
    "\n",
    "\n",
    "def per_category_scaler(X,min_dist_p_category=None):\n",
    "    \"\"\"Scale each feature in X by the maximum distance in this dimension which should still be considered as the same cluster in DBSCAN. Equivalent to eps parameter in DBSCAN, but separate for each dimension.\n",
    "    Args:\n",
    "    X (np.ndarray): Data to be scaled (n_samples, n_features)\n",
    "    min_dist_p_category (np.array, optional): DBSCAN eps parameter for each dimension Returns:\n",
    "    X_scaled (np.ndarray): scaled data \"\"\"\n",
    "\n",
    "if min_dist_p_category is None:\n",
    "    min_dist_p_category = np.ones(X.shape[1])\n",
    "\n",
    "X_scaled = np.zeros(X.shape) \n",
    "for i in range(X.shape[1]):\n",
    "    X_scaled[:,i] = X[:,i]/min_dist_p_category[i]\n",
    "return X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        ID         day CustomLocation       x  \\\n",
      "0     2F18EA72-F285-48E4-8DCA-D1CF1CF204C9  2024-08-29           True  760477   \n",
      "1     B719F193-8FDE-4ABA-BBC9-94A346970A5A  2024-08-29          False  696943   \n",
      "2     551b89e9-d0a5-4292-b3c2-f7b1fcf14117  2024-08-29          False  683823   \n",
      "3     FF798AED-00D2-43C3-BC19-D10740F6C728  2024-08-29           True  609829   \n",
      "4     60415511-7c80-4553-8fd5-2617f362c7a8  2024-08-29          False  595579   \n",
      "...                                    ...         ...            ...     ...   \n",
      "5034  84895b8e-c679-4f6d-a381-9213814f59d0  2024-09-09          False  684594   \n",
      "5035  074efa02-5e98-463c-8481-8c01dbc96dab  2024-09-09          False  679648   \n",
      "5036  f5f5387c-7d9a-4734-b8b4-e76e86cf6d3b  2024-09-09           True  583262   \n",
      "5037  58160616-f3b5-49ef-a6c7-69aaa309aac0  2024-09-09          False  541960   \n",
      "5038  58160616-f3b5-49ef-a6c7-69aaa309aac0  2024-09-09          False  541960   \n",
      "\n",
      "           y                                Time  size CustomTime  \\\n",
      "0     259667 2024-08-29 01:41:40.569999933+00:00    12      False   \n",
      "1     261949 2024-08-29 07:50:37.092000008+00:00    10      False   \n",
      "2     247646 2024-08-29 11:48:44.217000008+00:00    11      False   \n",
      "3     267793 2024-08-29 11:50:54.263999939+00:00    15      False   \n",
      "4     248815 2024-08-29 13:24:10.963999987+00:00    11      False   \n",
      "...      ...                                 ...   ...        ...   \n",
      "5034  248532 2024-09-09 19:45:00.536000013+00:00    14      False   \n",
      "5035   58971 2024-09-09 21:42:10.282999992+00:00    13      False   \n",
      "5036  113397           2024-09-09 10:55:54+00:00    10       True   \n",
      "5037  150689           2024-09-09 11:50:00+00:00    11       True   \n",
      "5038  150689           2024-09-09 11:50:00+00:00    10       True   \n",
      "\n",
      "                          SubmissionTime   OsVersion  ...         Time_m5  \\\n",
      "0    2024-08-29 01:42:17.053999901+00:00  ios-17.5.1  ...  20240829014000   \n",
      "1    2024-08-29 07:50:41.809000015+00:00  ios-17.5.1  ...  20240829075000   \n",
      "2    2024-08-29 11:48:48.456000090+00:00         nan  ...  20240829114500   \n",
      "3    2024-08-29 11:51:08.565999985+00:00  ios-17.6.1  ...  20240829115000   \n",
      "4    2024-08-29 13:24:37.214999914+00:00         nan  ...  20240829132000   \n",
      "...                                  ...         ...  ...             ...   \n",
      "5034 2024-09-09 19:45:25.476999998+00:00         nan  ...  20240909194500   \n",
      "5035 2024-09-09 21:43:02.165999889+00:00         nan  ...  20240909214000   \n",
      "5036 2024-09-10 04:55:05.713999987+00:00         nan  ...  20240909105500   \n",
      "5037 2024-09-10 06:25:53.158999920+00:00         nan  ...  20240909114500   \n",
      "5038 2024-09-10 06:28:27.029999971+00:00         nan  ...  20240909114500   \n",
      "\n",
      "             Time_p5  Flag_30min N_day_ID_custom Flag_day_ID_3_custom  \\\n",
      "0     20240829014500           0               1                    0   \n",
      "1     20240829075500           0               1                    0   \n",
      "2     20240829115000           0               1                    0   \n",
      "3     20240829115500           0               1                    0   \n",
      "4     20240829132500           0               1                    0   \n",
      "...              ...         ...             ...                  ...   \n",
      "5034  20240909195000           0               1                    0   \n",
      "5035  20240909214500           0               1                    0   \n",
      "5036  20240909110000           1               1                    0   \n",
      "5037  20240909115500           1               3                    0   \n",
      "5038  20240909115500           1               3                    0   \n",
      "\n",
      "     Flag_2min_ID_S_XL  Flag_1h_ID_3size_XL Flag_blacklist N_day_ID  \\\n",
      "0                    0                    0              0        1   \n",
      "1                    0                    0              0        1   \n",
      "2                    0                    0              0        1   \n",
      "3                    0                    0              0        1   \n",
      "4                    0                    0              0        1   \n",
      "...                ...                  ...            ...      ...   \n",
      "5034                 0                    0              0        1   \n",
      "5035                 0                    0              0        1   \n",
      "5036                 0                    0              0        1   \n",
      "5037                 0                    0              0        3   \n",
      "5038                 0                    0              0        3   \n",
      "\n",
      "     Flag_N_day_ID_4  \n",
      "0                  0  \n",
      "1                  0  \n",
      "2                  0  \n",
      "3                  0  \n",
      "4                  0  \n",
      "...              ...  \n",
      "5034               0  \n",
      "5035               0  \n",
      "5036               0  \n",
      "5037               0  \n",
      "5038               0  \n",
      "\n",
      "[5039 rows x 50 columns]\n"
     ]
    }
   ],
   "source": [
    "print(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 2: add radar variables\n",
    "- Change ls_rad_var depending on your needs\n",
    "- Create the sbatch files\n",
    "- They each call add_rad_var_crowd.py where time_1_list and time_2_list can be adapted. Note that time ranges larger than 15 minutes should be added beforehand in raw_data\n",
    "- Change the .csv filename in add_rad_var_crowd.py L29\n",
    "- Run the sbatch\n",
    "- add_rad_var_crowd.py calls prepare_gridded_radar_data_from_zip from get_archived_data.py (change the path if necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create sbatchs to add radar variables\n",
    "#Run the generated sbatchs in command line (call add_rad_var_crowd.py)\n",
    "\n",
    "ls_rad_var = ['BZC','MZC','CZC','EZC45','EZC50','HZT']\n",
    "date_from = \"2024-08-29\"\n",
    "date_to = \"2024-09-10\"\n",
    "\n",
    "with open('crowd_sbatch', 'r') as file:\n",
    "    # read a list of lines into data\n",
    "    data = file.readlines()\n",
    "\n",
    "for rad_var in ls_rad_var:\n",
    "    data[10] = '#SBATCH --job-name=crowd%s_%s_%s\\n' % (rad_var, date_from, date_to)\n",
    "    data[15] = 'srun python3 add_rad_var_crowd.py %s %s %s\\n' % (rad_var, date_from, date_to)\n",
    "    data[16] = 'cd /users/mryan/Code\\n'\n",
    "    #data[17] = 'find . -type f -name \\'%s%s*\\' -delete\\n' % (rad_var,yr)\n",
    "    #data[17] = 'find . -type f -name \\'%s*\\' -delete\\n' % (rad_var)\n",
    "\n",
    "    with open('crowd_sbatch_%s_%s_%s' % (rad_var, date_from, date_to), 'w') as file:\n",
    "        file.writelines(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: group all radar variables in a single dataframe\n",
    "Also compute maximum of radar variable over time window.\n",
    "The reflectivity filter of (Barras et al., 2019) is equivalent to maxCZC_t15_r4 >= 35 (dBZ)\n",
    "- Uncomment the list and loop over name_ to concatenate multiple time periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'crowd_CZC_2024-08-29_2024-09-10.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m\n\u001b[1;32m     12\u001b[0m crowd \u001b[38;5;241m=\u001b[39m cr\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rad_var \u001b[38;5;129;01min\u001b[39;00m ls_rad_var:    \n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m#df = pd.read_csv('crowd_%s_%s_%s.csv' % (rad_var,date1[0],date2[0])).drop(columns=['Unnamed: 0'])\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# name_ = ['crowd_%s_%s_%s.csv' % (rad_var,d1,d2) for d1, d2 in zip(date1[1:],date2[1:])]\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# for n in name_:\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m#     df_ = pd.read_csv(n).drop(columns=['Unnamed: 0'])\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m#     df = df.append(df_,ignore_index=True)\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcrowd_\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mrad_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdate_from\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdate_to\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnnamed: 0\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     21\u001b[0m     fields \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     22\u001b[0m     matching \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([s \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m fields \u001b[38;5;28;01mif\u001b[39;00m rad_var \u001b[38;5;129;01min\u001b[39;00m s \u001b[38;5;129;01mor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m ([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime\u001b[39m\u001b[38;5;124m\"\u001b[39m])])\n",
      "File \u001b[0;32m~/micromamba/envs/jerome/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/jerome/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/micromamba/envs/jerome/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/jerome/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/micromamba/envs/jerome/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'crowd_CZC_2024-08-29_2024-09-10.csv'"
     ]
    }
   ],
   "source": [
    "# Concatenate radar variables in single dataframe\n",
    "\n",
    "ls_time = ['5', '10', '15']\n",
    "ls_radius = ['0', '2', '4']\n",
    "# date1 = ['2015-01-01','2017-01-01','2019-01-01','2021-01-01','2022-01-01','2023-01-01']\n",
    "# date2 = ['2017-01-01','2019-01-01','2021-01-01','2022-01-01','2023-01-01','2024-01-01']\n",
    "date_from = \"2024-08-29\"\n",
    "date_to = \"2024-09-10\"\n",
    "ls_rad_var = ['CZC','BZC','MZC','EZC45','EZC50','HZT']\n",
    "\n",
    "cr = pd.read_csv('crowd_reports_%s_%s.csv' % (date_from, date_to)).drop(columns=['Unnamed: 0'])\n",
    "crowd = cr.copy()\n",
    "for rad_var in ls_rad_var:    \n",
    "    #df = pd.read_csv('crowd_%s_%s_%s.csv' % (rad_var,date1[0],date2[0])).drop(columns=['Unnamed: 0'])\n",
    "    # name_ = ['crowd_%s_%s_%s.csv' % (rad_var,d1,d2) for d1, d2 in zip(date1[1:],date2[1:])]\n",
    "    # for n in name_:\n",
    "    #     df_ = pd.read_csv(n).drop(columns=['Unnamed: 0'])\n",
    "    #     df = df.append(df_,ignore_index=True)\n",
    "    df = pd.read_csv('crowd_%s_%s_%s.csv' % (rad_var,date_from,date_to)).drop(columns=['Unnamed: 0'])\n",
    "\n",
    "    fields = df.columns.values.tolist()\n",
    "    matching = sorted([s for s in fields if rad_var in s or s in ([\"x\",\"y\", \"Time\"])])\n",
    "    df = df[matching]\n",
    "    crowd = pd.merge(crowd, df, how='left', on=[\"x\",\"y\", \"Time\"])\n",
    "\n",
    "# Compute maximum of radar variable over previous and next n time steps (time window)\n",
    "# To get reflectivity filter of (Barras et al., 2019): maxCZC_t15_r4 >= 35 (dBZ)\n",
    "for c in ls_rad_var:\n",
    "    for r in ls_radius:\n",
    "        n_ts = 'max%s_ts_r%s' % (c,r)\n",
    "        crowd = crowd.rename(columns = {'max%s_rad%s_ts' % (c,r): n_ts})\n",
    "        for t in ls_time:\n",
    "            n_ = 'max%s_t%s_r%s' % (c,t,r)\n",
    "            crowd[n_] = crowd[[\"max%s_rad%s_m%s\" % (c,r,t) , \"max%s_rad%s_p%s\" % (c,r,t)]].max(axis=1)\n",
    "\n",
    "\n",
    "crowd = crowd.drop_duplicates(subset=[\"ID\", \"x\",\"y\", \"Time\", \"size\"])\n",
    "\n",
    "# Save the crowdsourced data with radar variables\n",
    "crowd.to_csv('crowd_complete_%s_%s.csv' % (date_from,date_to))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: append the new data to existing one and select relevant fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate new data (crowd) with existing (old) (change name of df1 if necessary)\n",
    "old = pd.read_csv('crowd_complete_2015-05-12_2023-08-31.csv').drop(columns=['Unnamed: 0'])\n",
    "#new = pd.read_csv('crowd_complete_%s_%s.csv' % (date_from,date_to)).drop(columns=['Unnamed: 0'])\n",
    "\n",
    "new = crowd\n",
    "old = old.append(new,ignore_index=True)\n",
    "old = old.drop_duplicates(subset=[\"ID\", \"x\",\"y\", \"Time\", \"size\"])\n",
    "old.to_csv('crowd_complete_2015-05-12_%s.csv' % (date_to))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13613/3236999012.py:1: DtypeWarning: Columns (10,11,12,14,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  t = pd.read_csv('crowd_complete_2015-05-12_2023-10-15.csv')\n"
     ]
    }
   ],
   "source": [
    "t = pd.read_csv('crowd_complete_2015-05-12_2023-10-15.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = t[['ID', 'day', 'CustomLocation', 'x', 'y', 'Time',\n",
    "       'size', 'CustomTime', 'SubmissionTime', 'OsVersion', 'AppVersion',\n",
    "       'Language', 'T_diff', 'size_text', 'size_text_new',\n",
    "       'size_range_all', 'size_range_all_num', 'month', 'year', 'Timer',\n",
    "       'xr', 'yr', 'Time_30s', 'Time_1min', 'Time_2min', 'Time_1h',\n",
    "       'x_1000m', 'x_2000m', 'y_1000m', 'y_2000m', 'Flag_30min', 'N_day_ID_custom',\n",
    "       'Flag_day_ID_3_custom', 'Flag_2min_ID_S_XL', 'Flag_1h_ID_3size_XL',\n",
    "       'Flag_blacklist', 'N_day_ID', 'Flag_N_day_ID_4','Timestamp',\n",
    "       'maxCZC_ts_r0', 'maxCZC_ts_r2', 'maxCZC_ts_r4',\n",
    "       'maxCZC_t5_r0','maxCZC_t10_r0', 'maxCZC_t15_r0',\n",
    "       'maxCZC_t5_r2', 'maxCZC_t10_r2','maxCZC_t15_r2',\n",
    "       'maxCZC_t5_r4', 'maxCZC_t10_r4', 'maxCZC_t15_r4',\n",
    "       'maxBZC_ts_r0','maxBZC_ts_r2', 'maxBZC_ts_r4',\n",
    "       'maxBZC_t5_r0', 'maxBZC_t10_r0','maxBZC_t15_r0',\n",
    "       'maxBZC_t5_r2', 'maxBZC_t10_r2', 'maxBZC_t15_r2',\n",
    "       'maxBZC_t5_r4', 'maxBZC_t10_r4', 'maxBZC_t15_r4',\n",
    "       'maxMZC_ts_r0','maxMZC_ts_r2', 'maxMZC_ts_r4',\n",
    "       'maxMZC_t5_r0','maxMZC_t10_r0', 'maxMZC_t15_r0',\n",
    "       'maxMZC_t5_r2', 'maxMZC_t10_r2','maxMZC_t15_r2',\n",
    "       'maxMZC_t5_r4', 'maxMZC_t10_r4', 'maxMZC_t15_r4',\n",
    "       'maxEZC45_ts_r0', 'maxEZC45_ts_r2', 'maxEZC45_ts_r4',\n",
    "       'maxEZC45_t5_r0', 'maxEZC45_t10_r0', 'maxEZC45_t15_r0',\n",
    "       'maxEZC45_t5_r2', 'maxEZC45_t10_r2', 'maxEZC45_t15_r2',\n",
    "       'maxEZC45_t5_r4', 'maxEZC45_t10_r4', 'maxEZC45_t15_r4',\n",
    "       'maxEZC50_ts_r0', 'maxEZC50_ts_r2', 'maxEZC50_ts_r4',\n",
    "       'maxEZC50_t5_r0', 'maxEZC50_t10_r0', 'maxEZC50_t15_r0',\n",
    "       'maxEZC50_t5_r2', 'maxEZC50_t10_r2', 'maxEZC50_t15_r2',\n",
    "       'maxEZC50_t5_r4', 'maxEZC50_t10_r4', 'maxEZC50_t15_r4',\n",
    "       'maxHZT_ts_r0', 'maxHZT_ts_r2', 'maxHZT_ts_r4',\n",
    "       'maxHZT_t5_r0', 'maxHZT_t10_r0', 'maxHZT_t15_r0',\n",
    "       'maxHZT_t5_r2', 'maxHZT_t10_r2', 'maxHZT_t15_r2',\n",
    "       'maxHZT_t5_r4', 'maxHZT_t10_r4', 'maxHZT_t15_r4']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.to_pickle('crowd_complete_2015-05-02_2023-10-15.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: create regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create numpy array mask from population density data (Swiss100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_file = 'STATPOP2021.csv'\n",
    "population = pd.read_csv(pop_file,sep=';')\n",
    "population['chx'] = np.round(population['E_KOORD']/1000)\n",
    "population['chy'] = np.round(population['N_KOORD']/1000)\n",
    "pop_sum = population.groupby(['chx','chy']).agg({'B21BTOT': 'sum'})\n",
    "chx_range = [2484, 2838]\n",
    "chy_range = [1073, 1299]\n",
    "\n",
    "chx = np.arange(chx_range[0],chx_range[1])\n",
    "chy = np.arange(chy_range[0],chy_range[1])\n",
    "\n",
    "pop = np.zeros((len(chy),len(chx)))\n",
    "for i in range(len(pop_sum)): #tqdm()\n",
    "    x_coord = chx==pop_sum.index.get_level_values('chx')[i]\n",
    "    y_coord = chy==pop_sum.index.get_level_values('chy')[i]\n",
    "    pop[y_coord, x_coord] = pop_sum.iloc[i]['B21BTOT']\n",
    "\n",
    "xx, yy = np.meshgrid(chx, chy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At least 100 people per km2\n",
    "p = np.where(pop < 100, 0, 1)\n",
    "\n",
    "# Apply 3x3 dilation\n",
    "kernel1 = cv2.getStructuringElement(cv2.MORPH_RECT,(3,3))\n",
    "c = p.astype('uint8')\n",
    "dilation = cv2.dilate(c,kernel1,iterations = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend to radar domain\n",
    "\n",
    "np.save('CH_pop_maskall100d33.npy',dilation)\n",
    "b = np.zeros((640, 710))\n",
    "ch_mask = np.load('CH_pop_maskall100d33.npy')\n",
    "t = np.flip(ch_mask,axis=0)\n",
    "for x in np.arange(181,181+226):\n",
    "    for y in np.arange(228,228+354):\n",
    "        b[x,y] = t[x-181,y-228]\n",
    "\n",
    "np.save('all100d33_area_mask_extend.npy',b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create numpy array mask from rectangle (ZRH region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.zeros((640, 710))\n",
    "for x in np.arange(215,215+25):\n",
    "    for y in np.arange(404,404+40):\n",
    "        b[x,y] = 1\n",
    "\n",
    "np.save('ZRH_area_mask_extend.npy',b) #660-700; 240-265"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create shapefile masks from numpy arrays for selecting reports using geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = 'all100d33'\n",
    "from PIL import Image\n",
    "pop_mask = np.load('%s_area_mask_extend.npy' % (region))\n",
    "t = pop_mask.copy() * 255\n",
    "n = np.flipud(t)\n",
    "#n = t\n",
    "img = Image.fromarray(n)\n",
    "img = img.convert(\"L\")\n",
    "img.save('%s_area_mask_extend.tif' % (region))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal, ogr, osr\n",
    "raster = gdal.Open('%s_area_mask_extend.tif' % (region))\n",
    "band = raster.GetRasterBand(1)\n",
    "\n",
    "\n",
    "shp_proj = osr.SpatialReference()\n",
    "proj = raster.GetProjection()\n",
    "shp_proj.ImportFromWkt(proj)\n",
    "\n",
    "output_file = '%s_area_mask_extend.shp' % (region)\n",
    "call_drive = ogr.GetDriverByName('ESRI Shapefile')\n",
    "create_shp = call_drive.CreateDataSource(output_file)\n",
    "shp_layer = create_shp.CreateLayer('layername', srs = shp_proj)\n",
    "new_field = ogr.FieldDefn(str('ID'), ogr.OFTInteger)\n",
    "shp_layer.CreateField(new_field)\n",
    "dst_field = shp_layer.GetLayerDefn().GetFieldIndex(\"ID_1\")\n",
    "\n",
    "gdal.Polygonize(band, None, shp_layer, dst_field , [], callback=None)\n",
    "del create_shp\n",
    "del raster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swiss100 region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'all100d33_area_mask_extend.shp'\n",
    "regions = gpd.read_file(output_file)\n",
    "mask = ((regions.area > 200) & (regions.area < 100000))\n",
    "all100d33_area_temp = regions.loc[mask]\n",
    "ind_list = [0,2,3]\n",
    "all100d33_area = all100d33_area_temp.iloc[ind_list].reset_index().drop(columns=['ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZRH region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'ZRH_area_mask_extend.shp'\n",
    "ZRH_area = gpd.read_file(output_file)\n",
    "ZRH_area = ZRH_area.drop([1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: create clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select reports according to region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports = pd.read_pickle('crowd_complete_2015-05-02_2023-10-15.pkl')\n",
    "\n",
    "# Remove reports with time delay > 30 min, in the blacklist or made by users with 4 reports or more per day\n",
    "reports = reports.loc[(reports['Flag_30min']==0) & (reports['Flag_blacklist']==0) & (reports['Flag_N_day_ID_4']==0)]\n",
    "\n",
    "# Remove \"no hail\" reports\n",
    "reports = cr.loc[~((cr['size']==0) | (cr['size']==10))].copy()\n",
    "\n",
    "# Transform reports coordinates into geopandas points\n",
    "gdf2 = gpd.GeoDataFrame(reports[['ID','x','y','Time','size','hour']], geometry=gpd.points_from_xy(reports.x_1000m/1000 - 255, reports.y_1000m/1000 + 160))\n",
    "\n",
    "# Select reports only within area\n",
    "points_within = gpd.sjoin(gdf2, all100d33_area, predicate='within')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#points_within = points_within.loc[points_within['size'].isin([12,13,14,15,16])]\n",
    "pts = points_within[['ID','x','y','Time','size','hour']]\n",
    "#pts = points_within[['ID_left','x','y','Time','size', 'hour']]\n",
    "#pts = pts.rename(columns={'ID_left': 'ID'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filt = reports.merge(pts, on=[\"x\",\"y\",\"Time\",'ID','size'], how='left', indicator=True)\n",
    "df = df_filt.loc[(df_filt['_merge'] == 'both') & (df_filt['Time_only'] >= datetime.time(6,0)) & (df_filt['Time_only'] <= datetime.time(21,15))]\n",
    "#df = df[['ID','x','y','Time','size','year']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split by year to run clustering algorithm\n",
    "years = df['year'].unique().tolist()\n",
    "\n",
    "df_dict = {elem : pd.DataFrame() for elem in years}\n",
    "\n",
    "for key in df_dict.keys():\n",
    "    df_dict[key] = df[:][df.year == key]\n",
    "\n",
    "#list of parameters for clustering algorithm\n",
    "list_eps_d = [8,12,16]\n",
    "list_min_s = [5]\n",
    "list_eps_t = [480,720,960]\n",
    "area = 'all100d33_2' #ZRH_2\n",
    "n_ = ['','_large']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stats = []\n",
    "path = 'C:\\\\Users\\Jérôme\\Documents\\PhD\\Crowdsourced data\\\\analysis\\clusters\\\\'\n",
    "\n",
    "size_all = df['size'].unique().tolist()\n",
    "df = df[['ID','x','y','Time','size','year']]\n",
    "    \n",
    "for n in n_:\n",
    "    name = area + n\n",
    "    if n == '_nomin':\n",
    "        sizes = [12,13,14,15,16]\n",
    "    if n == '_large':\n",
    "        sizes = [13,14,15,16]\n",
    "    if n == '':\n",
    "        sizes = size_all\n",
    "    temp = df.loc[df['size'].isin(sizes)]\n",
    "    \n",
    "    years = temp['year'].unique().tolist()\n",
    "\n",
    "    df_dict = {elem : pd.DataFrame() for elem in years}\n",
    "\n",
    "    for key in df_dict.keys():\n",
    "        df_dict[key] = temp[:][temp.year == key]\n",
    "    \n",
    "    for year in years:\n",
    "        data = df_dict[year]\n",
    "        data['x_km'] = data['x']/1000\n",
    "        data['y_km'] = data['y']/1000\n",
    "        data['Time_int'] = data['Time'].astype('int64')/10**9 #time in seconds\n",
    "        data['Time_int'] = data['Time_int'] - data['Time_int'].min()\n",
    "        X_temp = data[['x_km','y_km','Time_int']].copy()\n",
    "        cols = X_temp.columns.tolist()\n",
    "        cols = cols[-1:] + cols[:-1]\n",
    "        X_temp = X_temp[cols]\n",
    "        X = X_temp.to_numpy(copy=True)\n",
    "\n",
    "        for eps_d, eps_t in zip(list_eps_d,list_eps_t):\n",
    "                for min_s in list_min_s:\n",
    "                    t0 = time.time()\n",
    "                    st_dbscan = ST_DBSCAN(eps1 = eps_d, eps2 = eps_t, min_samples = min_s) # eps1 = distance in km, eps2 = time in seconds, min_samples = min number of reports to create cluster\n",
    "                    st_dbscan.fit(X)\n",
    "                    labels = st_dbscan.labels\n",
    "                    X_temp['labels'] = labels\n",
    "                    n = len(set(labels))\n",
    "\n",
    "                    # Number of clusters in labels, ignoring noise if present.\n",
    "                    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "                    n_noise_ = list(labels).count(-1)\n",
    "                    t1 = time.time()\n",
    "                    total = t1-t0\n",
    "                    print(\"Total time: %f\" % total)\n",
    "                    crowd_clust = pd.merge(data, X_temp, how='left', on=[\"x_km\",\"y_km\",\"Time_int\"])\n",
    "                    crowd_clust.to_pickle(path + 'st_dbscan_%s_%s_%skm_%ss_%sc.pkl' % (year, name, eps_d, eps_t, min_s))\n",
    "                    #stats.append({'year': year, 'eps_t': eps_t, 'eps_d': eps_d, 'min_size': min_s, 'n_clusters': n_clusters_, 'n_noise': n_noise_, 'time': total})\n",
    "            \n",
    "# st = pd.DataFrame.from_dict(stats)\n",
    "# st.to_csv('st_dbscan_%s_stats.csv' % (area))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the yearly files\n",
    "\n",
    "path = 'C:\\\\Users\\Jérôme\\Documents\\PhD\\Crowdsourced data\\\\analysis\\clusters\\\\'\n",
    "\n",
    "for n in n_:\n",
    "    name = area + n\n",
    "    for eps_d, eps_t in zip(list_eps_d,list_eps_t):\n",
    "        for min_s in list_min_s:\n",
    "            data = {}\n",
    "            for file in os.listdir(path):\n",
    "                 if file.endswith('%s_%skm_%ss_%sc.pkl' % (name, eps_d, eps_t, min_s)):\n",
    "                    with open(path+file, 'rb') as f:\n",
    "                        data[file] = pd.read_pickle(f)\n",
    "\n",
    "            temp = pd.concat(data.values(), ignore_index=True).drop_duplicates()\n",
    "            temp.to_pickle('st_dbscan_%s_%skm_%ss_%sc.pkl' % (name, eps_d, eps_t, min_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge clusters data back with reports data\n",
    "for n in n_:\n",
    "    name = area + n\n",
    "    for eps_d, eps_t in zip(list_eps_d,list_eps_t):\n",
    "        clusters = pd.read_pickle('st_dbscan_%s_%skm_%ss_5c.pkl' % (name,eps_d,eps_t))\n",
    "        clusters['Time'] = pd.to_datetime(clusters['Time'], format='%Y-%m-%d %H:%M:%S')\n",
    "        clusters['clust_no'] = 0\n",
    "        clusters.loc[clusters['labels'] == -1,'clust_no'] = 1\n",
    "        tot = pd.merge(reports, clusters, how='inner', on=[\"x\",\"y\",\"Time\",'ID','size'])\n",
    "        tot.to_pickle('crowdsourced_data_%s_%skm_%ss_5c.pkl' % (name,eps_d,eps_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: create consolidated dataframe with all reports and clusters information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = pd.read_pickle('crowd_complete_2015-05-02_2023-10-15.pkl')\n",
    "z['Time'] = pd.to_datetime(z['Time'])\n",
    "\n",
    "ls_cl = ['8km_480s','12km_720s','16km_960s']\n",
    "\n",
    "nm_list = ['all100d33_2','ZRH_2','all100d33_2_large','ZRH_2_large']\n",
    "nm_new_list = ['_all100d33_','_ZRH_','_all100d33_large_','_ZRH_large_']\n",
    "\n",
    "for nm, nm_new in zip(nm_list, nm_new_list):\n",
    "    for cl in ls_cl:\n",
    "        t = pd.read_pickle('crowdsourced_data_%s_%s_5c.pkl' % (nm,cl))\n",
    "        t = t.rename(columns={\"clust_no\": \"cl_flag\" + nm_new + cl, \"labels\": \"cl_label\" + nm_new + cl})\n",
    "        t2 = t[['ID','x','y','Time','size',\"cl_flag\" + nm_new + cl, \"cl_label\" + nm_new + cl]]\n",
    "        z = z.merge(t2, on=[\"x\",\"y\",\"Time\",'ID','size'], how='left', indicator=False)\n",
    "\n",
    "z['Flag_CZC'] = 0\n",
    "z.loc[z['maxCZC_t15_r4']<=35, 'Flag_CZC'] = 1\n",
    "z.to_pickle('crowd_consolidated_2015-05-02_2023-10-15.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "crowd_data = pd.read_pickle('crowd_consolidated_2015-05-02_2023-10-15.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the reports according to Barras et al, 2019 (https://doi.org/10.1175/BAMS-D-18-0090.1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>day</th>\n",
       "      <th>CustomLocation</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>Time</th>\n",
       "      <th>size</th>\n",
       "      <th>CustomTime</th>\n",
       "      <th>SubmissionTime</th>\n",
       "      <th>OsVersion</th>\n",
       "      <th>...</th>\n",
       "      <th>cl_label_all100d33_large_12km_720s</th>\n",
       "      <th>cl_flag_all100d33_large_16km_960s</th>\n",
       "      <th>cl_label_all100d33_large_16km_960s</th>\n",
       "      <th>cl_flag_ZRH_large_8km_480s</th>\n",
       "      <th>cl_label_ZRH_large_8km_480s</th>\n",
       "      <th>cl_flag_ZRH_large_12km_720s</th>\n",
       "      <th>cl_label_ZRH_large_12km_720s</th>\n",
       "      <th>cl_flag_ZRH_large_16km_960s</th>\n",
       "      <th>cl_label_ZRH_large_16km_960s</th>\n",
       "      <th>Flag_CZC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>97262821-DB21-4159-9449-8A909A8492E1</td>\n",
       "      <td>2015-05-15</td>\n",
       "      <td>False</td>\n",
       "      <td>683416</td>\n",
       "      <td>248333</td>\n",
       "      <td>2015-05-15 07:54:38+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>2015-05-15 07:54:46+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>09E914A4-19BE-4178-A0C5-D1933E14C59B</td>\n",
       "      <td>2015-05-15</td>\n",
       "      <td>True</td>\n",
       "      <td>694542</td>\n",
       "      <td>253769</td>\n",
       "      <td>2015-05-15 11:15:55+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>2015-05-15 11:16:08+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>09E914A4-19BE-4178-A0C5-D1933E14C59B</td>\n",
       "      <td>2015-05-15</td>\n",
       "      <td>True</td>\n",
       "      <td>691069</td>\n",
       "      <td>236399</td>\n",
       "      <td>2015-05-15 11:16:27+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>2015-05-15 11:16:41+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>09E914A4-19BE-4178-A0C5-D1933E14C59B</td>\n",
       "      <td>2015-05-19</td>\n",
       "      <td>True</td>\n",
       "      <td>717339</td>\n",
       "      <td>96360</td>\n",
       "      <td>2015-05-19 13:28:27+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>2015-05-19 13:28:41+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>730E8977-63F4-4D4D-AAE8-572E1F2D8F8C</td>\n",
       "      <td>2015-05-26</td>\n",
       "      <td>False</td>\n",
       "      <td>661207</td>\n",
       "      <td>270029</td>\n",
       "      <td>2015-05-26 10:03:47+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>2015-05-26 10:04:03+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347818</th>\n",
       "      <td>dc25a980-d843-4b37-818e-15d934dfa223</td>\n",
       "      <td>2023-10-14</td>\n",
       "      <td>False</td>\n",
       "      <td>501691</td>\n",
       "      <td>127662</td>\n",
       "      <td>2023-10-14 13:01:32.733999872+00:00</td>\n",
       "      <td>12</td>\n",
       "      <td>False</td>\n",
       "      <td>2023-10-14 13:01:45.296999936+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347819</th>\n",
       "      <td>EEF3BCF1-4A13-4979-B147-7DA10854B6FB</td>\n",
       "      <td>2023-10-14</td>\n",
       "      <td>False</td>\n",
       "      <td>662056</td>\n",
       "      <td>255923</td>\n",
       "      <td>2023-10-14 13:06:24.896000+00:00</td>\n",
       "      <td>12</td>\n",
       "      <td>False</td>\n",
       "      <td>2023-10-14 13:06:30.227000064+00:00</td>\n",
       "      <td>ios-17.0.3</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347821</th>\n",
       "      <td>D3B28BEB-106D-4620-8B27-91D1DE70C8D8</td>\n",
       "      <td>2023-10-14</td>\n",
       "      <td>False</td>\n",
       "      <td>604088</td>\n",
       "      <td>207339</td>\n",
       "      <td>2023-10-14 13:48:18.900000+00:00</td>\n",
       "      <td>14</td>\n",
       "      <td>False</td>\n",
       "      <td>2023-10-14 13:48:21.061000192+00:00</td>\n",
       "      <td>ios-17.0.3</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347823</th>\n",
       "      <td>c6a33e6f-465c-465f-8e9e-2c74dcf7b8ab</td>\n",
       "      <td>2023-10-14</td>\n",
       "      <td>True</td>\n",
       "      <td>641534</td>\n",
       "      <td>203376</td>\n",
       "      <td>2023-10-14 14:45:43+00:00</td>\n",
       "      <td>13</td>\n",
       "      <td>True</td>\n",
       "      <td>2023-10-14 14:46:01.864000+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347824</th>\n",
       "      <td>e0b250be-e07c-4f60-8bcf-a401e62fd6b1</td>\n",
       "      <td>2023-10-14</td>\n",
       "      <td>False</td>\n",
       "      <td>753687</td>\n",
       "      <td>226404</td>\n",
       "      <td>2023-10-14 15:16:51.088000+00:00</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>2023-10-14 15:16:58.415000064+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>201882 rows × 136 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          ID         day  CustomLocation  \\\n",
       "13      97262821-DB21-4159-9449-8A909A8492E1  2015-05-15           False   \n",
       "27      09E914A4-19BE-4178-A0C5-D1933E14C59B  2015-05-15            True   \n",
       "28      09E914A4-19BE-4178-A0C5-D1933E14C59B  2015-05-15            True   \n",
       "57      09E914A4-19BE-4178-A0C5-D1933E14C59B  2015-05-19            True   \n",
       "84      730E8977-63F4-4D4D-AAE8-572E1F2D8F8C  2015-05-26           False   \n",
       "...                                      ...         ...             ...   \n",
       "347818  dc25a980-d843-4b37-818e-15d934dfa223  2023-10-14           False   \n",
       "347819  EEF3BCF1-4A13-4979-B147-7DA10854B6FB  2023-10-14           False   \n",
       "347821  D3B28BEB-106D-4620-8B27-91D1DE70C8D8  2023-10-14           False   \n",
       "347823  c6a33e6f-465c-465f-8e9e-2c74dcf7b8ab  2023-10-14            True   \n",
       "347824  e0b250be-e07c-4f60-8bcf-a401e62fd6b1  2023-10-14           False   \n",
       "\n",
       "             x       y                                Time  size  CustomTime  \\\n",
       "13      683416  248333           2015-05-15 07:54:38+00:00     3       False   \n",
       "27      694542  253769           2015-05-15 11:15:55+00:00     2       False   \n",
       "28      691069  236399           2015-05-15 11:16:27+00:00     2       False   \n",
       "57      717339   96360           2015-05-19 13:28:27+00:00     3       False   \n",
       "84      661207  270029           2015-05-26 10:03:47+00:00     1       False   \n",
       "...        ...     ...                                 ...   ...         ...   \n",
       "347818  501691  127662 2023-10-14 13:01:32.733999872+00:00    12       False   \n",
       "347819  662056  255923    2023-10-14 13:06:24.896000+00:00    12       False   \n",
       "347821  604088  207339    2023-10-14 13:48:18.900000+00:00    14       False   \n",
       "347823  641534  203376           2023-10-14 14:45:43+00:00    13        True   \n",
       "347824  753687  226404    2023-10-14 15:16:51.088000+00:00    11       False   \n",
       "\n",
       "                             SubmissionTime   OsVersion  ...  \\\n",
       "13                2015-05-15 07:54:46+00:00         NaN  ...   \n",
       "27                2015-05-15 11:16:08+00:00         NaN  ...   \n",
       "28                2015-05-15 11:16:41+00:00         NaN  ...   \n",
       "57                2015-05-19 13:28:41+00:00         NaN  ...   \n",
       "84                2015-05-26 10:04:03+00:00         NaN  ...   \n",
       "...                                     ...         ...  ...   \n",
       "347818  2023-10-14 13:01:45.296999936+00:00         NaN  ...   \n",
       "347819  2023-10-14 13:06:30.227000064+00:00  ios-17.0.3  ...   \n",
       "347821  2023-10-14 13:48:21.061000192+00:00  ios-17.0.3  ...   \n",
       "347823     2023-10-14 14:46:01.864000+00:00         NaN  ...   \n",
       "347824  2023-10-14 15:16:58.415000064+00:00         NaN  ...   \n",
       "\n",
       "       cl_label_all100d33_large_12km_720s cl_flag_all100d33_large_16km_960s  \\\n",
       "13                                    NaN                               NaN   \n",
       "27                                    NaN                               NaN   \n",
       "28                                    NaN                               NaN   \n",
       "57                                    NaN                               NaN   \n",
       "84                                    NaN                               NaN   \n",
       "...                                   ...                               ...   \n",
       "347818                                NaN                               NaN   \n",
       "347819                                NaN                               NaN   \n",
       "347821                               -1.0                               1.0   \n",
       "347823                                NaN                               NaN   \n",
       "347824                                NaN                               NaN   \n",
       "\n",
       "        cl_label_all100d33_large_16km_960s cl_flag_ZRH_large_8km_480s  \\\n",
       "13                                     NaN                        NaN   \n",
       "27                                     NaN                        NaN   \n",
       "28                                     NaN                        NaN   \n",
       "57                                     NaN                        NaN   \n",
       "84                                     NaN                        NaN   \n",
       "...                                    ...                        ...   \n",
       "347818                                 NaN                        NaN   \n",
       "347819                                 NaN                        NaN   \n",
       "347821                                -1.0                        NaN   \n",
       "347823                                 NaN                        NaN   \n",
       "347824                                 NaN                        NaN   \n",
       "\n",
       "       cl_label_ZRH_large_8km_480s cl_flag_ZRH_large_12km_720s  \\\n",
       "13                             NaN                         NaN   \n",
       "27                             NaN                         NaN   \n",
       "28                             NaN                         NaN   \n",
       "57                             NaN                         NaN   \n",
       "84                             NaN                         NaN   \n",
       "...                            ...                         ...   \n",
       "347818                         NaN                         NaN   \n",
       "347819                         NaN                         NaN   \n",
       "347821                         NaN                         NaN   \n",
       "347823                         NaN                         NaN   \n",
       "347824                         NaN                         NaN   \n",
       "\n",
       "        cl_label_ZRH_large_12km_720s cl_flag_ZRH_large_16km_960s  \\\n",
       "13                               NaN                         NaN   \n",
       "27                               NaN                         NaN   \n",
       "28                               NaN                         NaN   \n",
       "57                               NaN                         NaN   \n",
       "84                               NaN                         NaN   \n",
       "...                              ...                         ...   \n",
       "347818                           NaN                         NaN   \n",
       "347819                           NaN                         NaN   \n",
       "347821                           NaN                         NaN   \n",
       "347823                           NaN                         NaN   \n",
       "347824                           NaN                         NaN   \n",
       "\n",
       "        cl_label_ZRH_large_16km_960s Flag_CZC  \n",
       "13                               NaN        0  \n",
       "27                               NaN        0  \n",
       "28                               NaN        0  \n",
       "57                               NaN        0  \n",
       "84                               NaN        0  \n",
       "...                              ...      ...  \n",
       "347818                           NaN        0  \n",
       "347819                           NaN        0  \n",
       "347821                           NaN        0  \n",
       "347823                           NaN        0  \n",
       "347824                           NaN        0  \n",
       "\n",
       "[201882 rows x 136 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = crowd_data.loc[(crowd_data['Flag_30min']==0) & (crowd_data['Flag_blacklist']==0) & (crowd_data['Flag_CZC']==0) &\n",
    "               ~((crowd_data['size']==0) | (crowd_data['size']==10))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further removes reports if same user submitted more than 4 on same day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = temp.loc[(temp['Flag_N_day_ID_4']==0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clusters were made from reports (excluding the no hail reports) which were already filtered according to Flag_30min, Flag_blacklist, Flag_N_day_ID_4, so those filters don't have to be specified again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only reports that were part of a cluster on the Swiss100 region, using a 12km distance parameter and 12 minutes time parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = crowd_data.loc[(crowd_data['cl_flag_all100d33_12km_720s']==0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further removes reports where maximum reflectivity (CZC) was < 35 dBZ within 4km radius and 15min time window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = temp.loc[temp['Flag_CZC'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only reports of hail larger than 2 cm that were part of a cluster on the ZRH region, using a 8km distance parameter and 8 minutes time parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2017"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = crowd_data.loc[(crowd_data['cl_flag_ZRH_large_8km_480s']==0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the number of different clusters on 2021-06-28 on the Swiss100 region, using a 16km distance parameter and 16 minutes time parameter (-1 labels correspond to report that were not part of a cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp.loc[(temp['cl_label_all100d33_16km_960s'] > -1) & (crowd_data['day'] == '2021-06-28'),'cl_label_all100d33_16km_960s'].unique())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
